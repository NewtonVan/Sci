# UNSUPERVISED REPRESENTATION LEARNING BY PREDICTING IMAGE ROTATIONS

***

## Motivation

***

* supervised feature learning has the main limitation of requiring intensive manual labeling effort, which is both **expensive** and **infeasible** to scale on the vast amount of visual data that are available today

* self-supervised learning defines an annotation free pretext task, using only the visual information present on the images or videos, in order to provide a **surrogate supervision** signal for feature learning

* The rationale behind such self-supervised tasks is that solving them will force the ConvNet to learn semantic image features that can be useful for other vision tasks

* training by recognize the geometric transformation

* > We argue that in order a ConvNet model to be able recognize the rotation transformation that was applied to an image it will require to understand the concept of the objects depicted in the image 

## Contributions

***

* Propose a new self-supervised task that is very simple and offers a powerful supervisory signal for semantic feature learning

* > We exhaustively evaluate our self-supervised method under various settings (e.g. semisupervised or transfer learning settings) and in various vision tasks

* > we show that for several important vision tasks, our self-supervised learning approach significantly narrows the gap between unsupervised and supervised feature learning.

## Method

***

### Overview

***

* a set of discrete geometric transformations $G=\{g(\cdot|y\}^K_{y=1}$ , where $g(\cdot|y)$ is the operator that applies to image $X$ the geometric transformation with label $y$ that yields the transformed image $X^y=g(X|y)$
* The ConvNet model $F(\cdot)$ gets as input an image $X^{y^*}$ (where the label $y^*$ is unknown to $F(\cdot)$) and yields as output a probability distribution over all possible geometric transformations: $F(X^{y^*}|\theta)=\{F^y(X^{y^*}|\theta)\}^K_{y=1}$ 
* Objective: $\min\limits_\theta\frac{1}{N}\sum\limits_{i=1}^{N}loss(X_i, \theta)$
* loss: $-\frac{1}{K}\sum\limits_{y=1}^K\log(F^y(g(X_i|y)|\theta))$

### CHOOSING GEOMETRIC TRANSFORMATIONS: IMAGE ROTATIONS

***

* proposed geometric transformations are the image rotations by multiple $90^\circ$
* Forcing the learning of semantic features
  * to localize salient objects in the image
  * recognize their orientation and object type
  * relate the object orientation with the dominant orientation that each type of object tends to be depicted within the available images
* Absence of low-level visual artifacts
  * that do not leave any easily detectable low-level visual artifacts that will lead the ConvNet to learn trivial features with no practical value for the vision perception tasks
* Well-posedness
  * there is usually no ambiguity of what is the rotation transformation 
* Implementing image rotations
  * use flip and transpose operations

### Discussion

***

* It has the same computational cost as supervised learning, similar training convergence speed
* can trivially adopt the efficient parallelization schemes devised for supervised learning on internet-scale data
* does not require any special image pre-processing routine in order to avoid learning trivial features
* achieve dramatic improvements on the unsupervised feature learning benchmarks

## Experiment

***

### CIFAR

***

* details:

  * > In our preliminary experiments we found that we get significant improvement when during training we train the network by feeding it all the four rotated copies of an image simultaneously instead of each time randomly sampling a single rotation transformation

* Evaluation of the learned feature hierarchies

  * > We observe that in all cases the feature maps generated by the 2nd conv. block (that actually has depth 6 in terms of the total number of conv. layer till that point) achieve the highest accuracy, i.e., between 88.26% and 89.06%.

  * > The features of the conv. blocks that follow the 2nd one gradually degrade the object recognition accuracy, which we assume is because they start becoming more and more **specific** on the self-supervised task of rotation prediction.

  * > that increasing the total depth of the RotNet models leads to increased object recognition performance by the feature maps generated by earlier layers 

  * > because increasing the depth of the model and thus the complexity of its head allows the features of earlier layers to be **less specific** to the rotation prediction task

* Exploring the quality of the learned features w.r.t. the number of recognized rotations

  * Implementation: image wrapping routine and crop only the central square image regions

  * > We observe that indeed for 4 discrete rotations (as we proposed) we achieve better object recognition performance than the 8 or 2 cases.

  * > Moreover, we observe that among the RotNet models trained with 2 discrete rotations, the RotNet model trained with 90◦ and 270◦ rotations achieves worse object recognition performance than the model trained with the 0◦ and 180◦ rotations

* Comparison against supervised and other unsupervised methods

  * > We observe that we improve over the prior unsupervised approaches and we achieve state-of-the-art results in CIFAR-10

  * > More notably, the accuracy gap between the RotNet based model and the fully supervised NIN model is very small, only 1.64 percentage points (92.80% vs 91.16%)

  * > We observe that fine-tuning the unsupervised learned features further improves the classification performance, thus reducing even more the gap with the supervised case.

* Correlation between object classification task and rotation prediction task

  * > We observe that, as the ability of the RotNet features for solving the rotation prediction task improves (i.e., as the rotation prediction accuracy increases), their ability to help solving the object recognition task improves as well (i.e., the object recognition accuracy also increases). 

  * > Furthermore, we observe that the object recognition accuracy converges fast w.r.t. the number of training epochs used for solving the pretext task of rotation prediction.

* Semi-supervised setting

  * > We observe that our unsupervised trained model exceeds in this semi-supervised setting the supervised model when the number of examples per category drops below 1000. 

  * > Furthermore, as the number of examples decreases, the performance gap in favor of our method is increased

### EVALUATION OF SELF-SUPERVISED FEATURES TRAINED IN IMAGENET

***

* ImageNet classification task:

  * > We observe that our approach surpasses all the other methods(unsupervised method) by a significant margin.

  * > Furthermore, our approach significantly narrows the performance gap between unsupervised features and supervised features

* Transfer learning evaluation on PASCAL VOC: In

  * > As with the ImageNet classification task, we outperform by significant margin all the competing unsupervised methods in all tested tasks, significantly narrowing the gap with the supervised case. 

* Places classification task

  * > even in this case our method manages to either surpass or achieve comparable results w.r.t. prior state-of-the-art unsupervised learning approaches.